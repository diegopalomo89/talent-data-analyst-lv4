{"cells":[{"cell_type":"code","source":["##################################################### Importação do arquivo #############################################################################\n\nurl = \"https://st-it-cloud-public.s3.amazonaws.com/people-v2_1E6.csv.gz\"\n\nfrom pyspark import SparkFiles\nspark.sparkContext.addFile(url)\ndfPeople = spark.read.csv(\"file://\"+SparkFiles.get(\"people-v2_1E6.csv.gz\"), header=True, inferSchema= True, sep=';')\n\ndfPeople.printSchema()\n\n##################################################### Higienização ##############################################################################\n\n#Higieniza a coluna documento\nfrom pyspark.sql.functions import regexp_replace, col\ndfPeople=dfPeople.withColumn(\"document\",regexp_replace(col(\"document\"), \"[\\-.(\\s+)(\\D+)]\", \"\"))\n\n#Extrai quantos dígitos cada valor da coluna documento possui, para identificar se é CPF ou CNPJ\nfrom pyspark.sql.functions import length\ndfPeople = dfPeople.withColumn(\"tamanho\", length(\"document\"))\n\nfrom pyspark.sql.functions import when\ndfPeople = dfPeople.withColumn(\"cpfCnpj\", when(dfPeople.tamanho == 11,\"CPF\")\n                                          .otherwise(\"CNPJ\"))\n\n#Remove as vírgulas dos registros de data\nfrom pyspark.sql.functions import regexp_replace, col\ndfPeople=dfPeople.withColumn(\"birthDate\",regexp_replace(col(\"birthDate\"), \",\", \"\"))\n\n#Para os casos aonde o formato da data está com 15 dígitos, contendo o nome do dia abreviado, é feita essa extração excluindo o dia abreviado e o espaço.Isso é uma etapa para o tratamento posterior, mais abaixo, para deixar todas as datas no mesmo formato, conforme desejado pela especificação.\nfrom pyspark.sql.functions import length, substring\ndfPeople=dfPeople.withColumn(\"birthDateSubstring\", substring('BirthDate',5,12))\n\nfrom pyspark.sql.functions import regexp_replace, col\ndfPeople=dfPeople.withColumn(\"birthDateSubstring\",regexp_replace(col(\"birthDateSubstring\"), \"[.(\\s+)]\", \"-\"))\n\n\n\n#Realiza a formatação para um padrão único, normalizando o dado que está em inúmeros formatos diferentes de data. É criado uma coluna temporária para cada formato a ser tratado e depois juntamos os valores tratados em coluna única através da função COALESCE.\n                             \n\nfrom pyspark.sql import functions as F\n\ndfPeople = dfPeople.withColumn(\n            'newDate',\n                F.to_date(\n                    F.unix_timestamp('birthDate', 'MM-dd-yyyy').cast('timestamp')))\n\ndfPeople = dfPeople.withColumn(\n            'newDate2',\n                F.to_date(\n                    F.unix_timestamp('birthDate', 'dd-MM-yyyy').cast('timestamp')))\n\n\ndfPeople = dfPeople.withColumn(\n            'newDate3',\n                F.to_date(\n                    F.unix_timestamp('birthDate', 'MMM/dd/yyyy').cast('timestamp')))\n\ndfPeople = dfPeople.withColumn(\n            'newDate4',\n                F.to_date(\n                    F.unix_timestamp('birthDate', 'dd-MMM-yyyy').cast('timestamp')))\n\n\ndfPeople = dfPeople.withColumn(\n            'newDate5',\n                F.to_date(\n                    F.unix_timestamp('birthDate', 'MM/dd/yyyy').cast('timestamp')))\n\ndfPeople = dfPeople.withColumn(\n            'newDate6',\n                F.to_date(\n                    F.unix_timestamp('birthDate', 'yyyyMMdd').cast('timestamp')))\n\ndfPeople = dfPeople.withColumn(\n            'newDate7',\n                F.to_date(\n                    F.unix_timestamp('birthDateSubstring', 'MMM-dd-yyyy').cast('timestamp')))\n\n\n                             \nfrom pyspark.sql.functions import coalesce\n    \ndfPeople=dfPeople.withColumn(\"birthDate\",coalesce(dfPeople.newDate,dfPeople.newDate2, dfPeople.newDate3, dfPeople.newDate4, dfPeople.newDate5, dfPeople.newDate6,dfPeople.newDate7)) \n\n\n#As duas colunas com informações trocadas para algumas linhas são a phoneNumber e a JobArea, porém o tratamento desta parte não será desenvolvido devido ao prazo de entrega\n\n#Homogeniza o padrão de valores para a coluna Estado\nfrom pyspark.sql.functions import when\ndfPeople=dfPeople.withColumn('state',\n when(dfPeople.state==('Santa Catarina'),regexp_replace(dfPeople.state,'Santa Catarina','SC')) \\\n   .when(dfPeople.state==('Mato Grosso do Sul'),regexp_replace(dfPeople.state,'Mato Grosso do Sul','MS')) \\\n   .when(dfPeople.state==('Goiás'),regexp_replace(dfPeople.state,'Goiás','GO')) \\\n   .when(dfPeople.state==('Mato Grosso'),regexp_replace(dfPeople.state,'Mato Grosso','MT')) \\\n   .when(dfPeople.state==('Ceará'),regexp_replace(dfPeople.state,'Ceará','CE')) \\\n   .when(dfPeople.state==('Espírito Santo'),regexp_replace(dfPeople.state,'Espírito Santo','ES')) \\\n   .when(dfPeople.state==('Piauí'),regexp_replace(dfPeople.state,'Piauí','PI')) \\\n   .when(dfPeople.state==('Paraná'),regexp_replace(dfPeople.state,'Paraná','PR')) \\\n   .when(dfPeople.state==('Alagoas'),regexp_replace(dfPeople.state,'Alagoas','AL')) \\\n   .when(dfPeople.state==('Bahia'),regexp_replace(dfPeople.state,'Bahia','BA')) \\\n   .when(dfPeople.state==('Roraima'),regexp_replace(dfPeople.state,'Roraima','RR')) \\\n   .when(dfPeople.state==('Distrito Federal'),regexp_replace(dfPeople.state,'Distrito Federal','DF')) \\\n   .when(dfPeople.state==('Pernambuco'),regexp_replace(dfPeople.state,'Pernambuco','PE')) \\\n   .when(dfPeople.state==('Amazonas'),regexp_replace(dfPeople.state,'Amazonas','AM')) \\\n   .when(dfPeople.state==('Acre'),regexp_replace(dfPeople.state,'Acre','AC')) \\\n   .when(dfPeople.state==('Rio Grande do Sul'),regexp_replace(dfPeople.state,'Rio Grande do Sul','RS'))\\\n   .when(dfPeople.state==('Rio Grande do Norte'),regexp_replace(dfPeople.state,'Rio Grande do Norte','RN'))\\\n   .when(dfPeople.state==('Sergipe'),regexp_replace(dfPeople.state,'Sergipe','SE')) \\\n   .when(dfPeople.state==('São Paulo'),regexp_replace(dfPeople.state,'São Paulo','SP')) \\\n   .when(dfPeople.state==('Rio de Janeiro'),regexp_replace(dfPeople.state,'Rio de Janeiro','RJ')) \\\n   .when(dfPeople.state==('Minas Gerais'),regexp_replace(dfPeople.state,'Minas Gerais','MG')) \\\n   .when(dfPeople.state==('Rondônia'),regexp_replace(dfPeople.state,'Rondônia','RO')) \\\n   .when(dfPeople.state==('Tocantins'),regexp_replace(dfPeople.state,'Tocantins','TO')) \\\n   .when(dfPeople.state==('Maranhão'),regexp_replace(dfPeople.state,'Maranhão','MA')) \\\n   .when(dfPeople.state==('Paraíba'),regexp_replace(dfPeople.state,'Paraíba','PB')) \\\n   .when(dfPeople.state==('Pará'),regexp_replace(dfPeople.state,'Pará','PA')) \\\n   .when(dfPeople.state==('Amapá'),regexp_replace(dfPeople.state,'Amapá','AP')) \\\n   .otherwise(dfPeople.state))\n\n\n\n################################################### REPORTS ###########################################################################################\n\n#Demonstra os 5 PFs com maior totalSpent\nfrom pyspark.sql.functions import col\ndfpeopleCpfs=dfPeople.filter(dfPeople.cpfCnpj=='CPF')\ndfPeoplePFs=dfpeopleCpfs.groupBy(\"document\").sum(\"totalSpent\")\ndfPeoplePFs.orderBy(col((\"sum(totalSpent)\")).desc()).show(5)\n\n#Gasto médio por estado\nimport pyspark.sql.functions as F\ndfMediaEstado=dfPeople.groupBy(\"state\").agg(F.mean(\"totalSpent\"))\ndfMediaEstado.show()\n\n#Gasto médio por área\nimport pyspark.sql.functions as F\ndfMediaJobArea=dfPeople.groupBy(\"jobArea\").agg(F.mean(\"totalSpent\"))\ndfMediaJobArea.show()\n\n#PF que gastou menos\nfrom pyspark.sql.functions import col\ndfPeoplePFsLess = dfPeoplePFs.filter((col(\"sum(totalSpent)\").isNotNull()))\ndfPeoplePFs.orderBy(col((\"sum(totalSpent)\")).asc()).show(1)\n\n#Quantos nomes e documentos repetidos\nfrom pyspark.sql.functions import col\ndfPeopleNomesRep=dfPeople.groupBy(\"document\", \"name\").count()\ndfPeopleNomesRep=dfPeopleNomesRep.withColumnRenamed(\"count\",\"countRenamed\")\ndfPeopleNomesRep=dfPeopleNomesRep.withColumn(\"countInt\",dfPeopleNomesRep.countRenamed.cast('integer'))\ndfPeopleNomesRepFil = dfPeopleNomesRep.filter(dfPeopleNomesRep.countInt > 1)\ndfPeopleNomesRepFil.select(count('document')).show()\n\n#Quantas linhas existem no dataset\ndfPeople.select(count(\"document\")).show()\n\n\n                \n\n\n\n\n\n################################################# GERAÇÃO DOS ARQUIVOS ##################################################################################\ndfPeople=dfPeople.drop(\"birthDateSubstring\")\\\n                 .drop(\"newDate\")\\\n                 .drop(\"newDate2\")\\\n                 .drop(\"newDate3\")\\\n                 .drop(\"newDate4\")\\\n                 .drop(\"newDate5\")\\\n                 .drop(\"newDate6\")\\\n                 .drop(\"newDate7\")\\\n                 .drop(\"tamanho\")\n\n\ndfPeople.write.mode('overwrite').partitionBy(\"state\").parquet(\"/temp/spark_output/problema2_normalizado\")\ndfPeople.write.format('csv').option('header',True).mode('overwrite').partitionBy(\"birthDate\").option('sep',';').save('file:///home/tangr/output2.csv')\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2c6b2993-5611-436d-a69b-ef0ed110bcd7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">root\n |-- document: string (nullable = true)\n |-- name: string (nullable = true)\n |-- job: string (nullable = true)\n |-- jobArea: string (nullable = true)\n |-- jobType: string (nullable = true)\n |-- phoneNumber: string (nullable = true)\n |-- birthDate: string (nullable = true)\n |-- city: string (nullable = true)\n |-- state: string (nullable = true)\n |-- totalSpent: double (nullable = true)\n\n+-----------+------------------+\n|   document|   sum(totalSpent)|\n+-----------+------------------+\n|78155376770|1973.1100000000001|\n|57913684291|           1889.39|\n|08174237704|           1870.75|\n|70795760671|1857.1100000000001|\n|31307158277|            1854.8|\n+-----------+------------------+\nonly showing top 5 rows\n\n+-----+------------------+\n|state|   avg(totalSpent)|\n+-----+------------------+\n|   SC|501.34561245212547|\n|   RO|498.23731784934995|\n|   PI| 499.0765211522321|\n|   AM| 498.7706629805852|\n|   RR|500.95639783978345|\n|   GO|501.29249260830375|\n|   TO|501.90156334817084|\n|   MT|500.26461002714404|\n|   SP|498.69447971507907|\n|   ES|501.63174747811667|\n|   PB|499.20039918879166|\n|   RS| 500.6309834299535|\n|   MS| 499.5097414101175|\n|   AL|500.51551942023275|\n|   MG| 499.5320941292197|\n|   PA|   499.23084708669|\n|   BA| 498.7741403086009|\n|   SE|498.24009925154405|\n|   PE|502.40577268067875|\n|   CE|499.30029952678933|\n+-----+------------------+\nonly showing top 20 rows\n\n+------------------+---------------+\n|           jobArea|avg(totalSpent)|\n+------------------+---------------+\n|+55 (81) 3777-4847|          43.85|\n|+55 (91) 8074-1482|         244.37|\n|    (69) 2266-7829|          48.84|\n|    (93) 1477-7460|         670.78|\n|   (69) 00873-3524|         862.06|\n|   (00) 87256-8486|         994.55|\n|   (42) 75457-9490|         421.28|\n|+55 (82) 7251-0310|          234.4|\n|    (90) 7060-8461|         583.81|\n|   (10) 93704-1423|         782.37|\n|+55 (69) 5996-0558|         159.23|\n|+55 (48) 6741-1654|         994.87|\n|+55 (52) 7788-4333|         318.88|\n|+55 (66) 4496-0159|         530.06|\n|    (18) 8851-4313|         903.48|\n|+55 (63) 5153-1855|         312.83|\n|    (63) 2918-2453|         132.59|\n|    (60) 8307-0036|         693.49|\n|+55 (04) 9995-7226|         762.74|\n|+55 (53) 4440-7248|         104.68|\n+------------------+---------------+\nonly showing top 20 rows\n\n+-----------+---------------+\n|   document|sum(totalSpent)|\n+-----------+---------------+\n|32737615984|           null|\n+-----------+---------------+\nonly showing top 1 row\n\n+---------------+\n|count(document)|\n+---------------+\n|              0|\n+---------------+\n\n+---------------+\n|count(document)|\n+---------------+\n|        1000000|\n+---------------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- document: string (nullable = true)\n-- name: string (nullable = true)\n-- job: string (nullable = true)\n-- jobArea: string (nullable = true)\n-- jobType: string (nullable = true)\n-- phoneNumber: string (nullable = true)\n-- birthDate: string (nullable = true)\n-- city: string (nullable = true)\n-- state: string (nullable = true)\n-- totalSpent: double (nullable = true)\n\n+-----------+------------------+\n   document|   sum(totalSpent)|\n+-----------+------------------+\n78155376770|1973.1100000000001|\n57913684291|           1889.39|\n08174237704|           1870.75|\n70795760671|1857.1100000000001|\n31307158277|            1854.8|\n+-----------+------------------+\nonly showing top 5 rows\n\n+-----+------------------+\nstate|   avg(totalSpent)|\n+-----+------------------+\n   SC|501.34561245212547|\n   RO|498.23731784934995|\n   PI| 499.0765211522321|\n   AM| 498.7706629805852|\n   RR|500.95639783978345|\n   GO|501.29249260830375|\n   TO|501.90156334817084|\n   MT|500.26461002714404|\n   SP|498.69447971507907|\n   ES|501.63174747811667|\n   PB|499.20039918879166|\n   RS| 500.6309834299535|\n   MS| 499.5097414101175|\n   AL|500.51551942023275|\n   MG| 499.5320941292197|\n   PA|   499.23084708669|\n   BA| 498.7741403086009|\n   SE|498.24009925154405|\n   PE|502.40577268067875|\n   CE|499.30029952678933|\n+-----+------------------+\nonly showing top 20 rows\n\n+------------------+---------------+\n           jobArea|avg(totalSpent)|\n+------------------+---------------+\n+55 (81) 3777-4847|          43.85|\n+55 (91) 8074-1482|         244.37|\n    (69) 2266-7829|          48.84|\n    (93) 1477-7460|         670.78|\n   (69) 00873-3524|         862.06|\n   (00) 87256-8486|         994.55|\n   (42) 75457-9490|         421.28|\n+55 (82) 7251-0310|          234.4|\n    (90) 7060-8461|         583.81|\n   (10) 93704-1423|         782.37|\n+55 (69) 5996-0558|         159.23|\n+55 (48) 6741-1654|         994.87|\n+55 (52) 7788-4333|         318.88|\n+55 (66) 4496-0159|         530.06|\n    (18) 8851-4313|         903.48|\n+55 (63) 5153-1855|         312.83|\n    (63) 2918-2453|         132.59|\n    (60) 8307-0036|         693.49|\n+55 (04) 9995-7226|         762.74|\n+55 (53) 4440-7248|         104.68|\n+------------------+---------------+\nonly showing top 20 rows\n\n+-----------+---------------+\n   document|sum(totalSpent)|\n+-----------+---------------+\n32737615984|           null|\n+-----------+---------------+\nonly showing top 1 row\n\n+---------------+\ncount(document)|\n+---------------+\n              0|\n+---------------+\n\n+---------------+\ncount(document)|\n+---------------+\n        1000000|\n+---------------+\n\n</div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Data Analyst ST Item 2","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":3265419599146856}},"nbformat":4,"nbformat_minor":0}
